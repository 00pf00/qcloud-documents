## 操作场景

使用腾讯云容器服务 [云原生监控](https://cloud.tencent.com/document/product/457/49889)，如何快速将之前自建的 Prometheus 迁移到云原生监控？
云原生监控兼容 Prometheus 与 Grafana 的 API，同时也兼容主流 prometheus-operator 的 CRD 用法，提供了极大的灵活性与扩展性，可以结合 Prometheus 开源生态的工具解锁更多高级玩法。
本文将介绍如何通过一些辅助脚本和迁移工具，实现快速将自建 Prometheus 迁移到云原生监控。



## 前提条件



已在自建 Prometheus 集群的一个节点上安装 [Kubectl](https://kubernetes.io/zh/docs/tasks/tools/install-kubectl/) 并配置好 kubeconfig，保证通过 Kubectl 能够管理集群。



## 操作步骤


### 迁移动态采集配置

如果使用 prometheus-operator，往往需使用 ServiceMonitor 和 PodMonitor 这类 CRD 资源来动态添加采集配置，云原生监控也支持该用法，若只是将当前集群的 prometheus-operator 迁移到云原生监控，并未迁移集群，则无需迁移动态配置，只需使用云原生监控关联自建集群，之前创建的 ServiceMonitor 和 PodMonitor 资源就会自动在云原生监控中生效。

如需跨集群迁移，可以将之前集群的 CRD 资源导出，并选择性的在被关联的云原生监控的集群中重新应用。以下为您提供如何在自建 Prometheus 集群中批量导出 ServiceMonitor 和 PodMonitor 的示例。



1. 创建脚本 `prom-backup.sh`，脚本内容如下：
<dx-codeblock>
:::  bash
_ns_list=$(kubectl get ns | awk '{print $1}' | grep -v NAME)
count=0
declare -a types=("servicemonitors.monitoring.coreos.com" "podmonitors.monitoring.coreos.com")
for _ns in ${_ns_list}; do
    ## loop for types
    for _type in "${types[@]}"; do
	    echo "Backup type [namespace: ${_ns}, type: ${_type}]."
        _item_list=$(kubectl -n ${_ns} get ${_type} | grep -v NAME | awk '{print $1}' )
	    ## loop for items
    	for _item in ${_item_list}; do
	        _file_name=./${_ns}_${_type}_${_item}.yaml
		    echo "Backup kubernetes config yaml [namespace: ${_ns}, type: ${_type}, item: ${_item}] to file: ${_file_name}"
		    kubectl -n ${_ns} get ${_type} ${_item} -o yaml > ${_file_name}
		    count=$[count + 1]
		    echo "Backup No.${count} file done."
        done;
    done;
done;
:::
</dx-codeblock>
2. 执行以下命令运行 `prom-backup.sh` 脚本：
```bash
bash prom-backup.sh
```
3.  `prom-backup.sh` 脚本会将每个 ServiceMonitor 与 PodMonitor 资源导出成单独的 YAML 文件。可执行以下 `ls` 命令查看输出的文件列表，示例如下：
```bash
$ ls
kube-system_servicemonitors.monitoring.coreos.com_kube-state-metrics.yaml
kube-system_servicemonitors.monitoring.coreos.com_node-exporter.yaml
monitoring_servicemonitors.monitoring.coreos.com_coredns.yaml
monitoring_servicemonitors.monitoring.coreos.com_grafana.yaml
monitoring_servicemonitors.monitoring.coreos.com_kube-apiserver.yaml
monitoring_servicemonitors.monitoring.coreos.com_kube-controller-manager.yaml
monitoring_servicemonitors.monitoring.coreos.com_kube-scheduler.yaml
monitoring_servicemonitors.monitoring.coreos.com_kube-state-metrics.yaml
monitoring_servicemonitors.monitoring.coreos.com_kubelet.yaml
monitoring_servicemonitors.monitoring.coreos.com_node-exporter.yaml
```
4. 你可以自行筛选和修改，将 YAML 文件重新应用到被关联的云原生监控的集群中（请勿应用已经存在或功能相同的采集规则），云原生监控会自动感知这部分动态采集规则并进行采集。
>?如果后续需要增加 ServiceMonitor 或 PodMonitor，可以通过 TKE 控制台进行可视化添加，也可以脱离控制台直接用 YAML 创建，用法与社区的 CRD 完全兼容。



### 迁移静态采集配置

若自建的 Prometheus 系统直接使用 Prometheus 原生配置文件，只需在 TKE 控制台进行简单的几步操作，即可将其转换为云原生监控的 RawJob，使其兼容 Prometheus 原生配置文件的 `scrape_configs` 配置项。

1. 登录 [容器服务控制台](https://console.cloud.tencent.com/tke2)。
2. 在左侧菜单栏中单击 【云原生监控】进入云原生监控页面。
3. 单击需要配置的云原生监控 ID/名称，进入基本页面。
4. 选择【关联集群】页签，在对应的集群右侧【操作】列项下单击【数据采集配置】。
![](https://main.qcloudimg.com/raw/fb5c0c610cec5b6366736c57af7cfced.png)
5. 在【RawJob】页面下单击【新增】，将原生 Prometheus 配置文件中的 Job 配置复制粘贴到此。
![](https://main.qcloudimg.com/raw/9bd9d5363e1e60990997c292c2368d2e.png)
6. 可以将所有需要导入的 Job 数组都直接粘贴进来，单击【确定】后会自动拆分成多个 RawJob，名称为每个 Job 的 `job_name` 字段。



### 迁移全局配置

云原生监控提供 Prometheus CRD 资源，可以通过修改该资源来修改全局配置。

1. 执行以下命令查看 Prometheus 相关信息。
```bash
$ kubectl get ns
prom-fnc7bvu9     Active   13m
$ kubectl -n prom-fnc7bvu9 get prometheus
NAME               VERSION   REPLICAS   AGE
tke-cls-hha93bp9                        11m
$ kubectl -n prom-fnc7bvu9 edit prometheus tke-cls-hha93bp9
```
2. 执行以下命令，修改 Prometheus 相关配置。
```bash
$ kubectl -n prom-fnc7bvu9 edit prometheus tke-cls-hha93bp9
```
 其中：
 - 修改 `scrapeInterval`，可修改默认的采集抓取间隔时长（默认为15s）。
 - 修改 `externalLabels`，可为所有时序数据增加默认的 label 标识。




### 迁移聚合配置

Prometheus 的聚合配置，无论是原始的 [Recording rules](https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/) 静态配置或是 [PrometheusRule](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#prometheusrule) 动态配置，每条规则的格式都相同。

1. 登录 [容器服务控制台](https://console.cloud.tencent.com/tke2)。
2. 在左侧菜单栏中单击 【云原生监控】进入云原生监控页面。
3. 单击需要配置的云原生监控 ID/名称，进入基本页面。
4. 选择【聚合规则】>【新建聚合规则】，创建聚合规则：
5. 使用 PrometheusRule 的格式，将每条规则粘贴到 `groups` 数组里即可。如下图所示：
![](https://main.qcloudimg.com/raw/90fe3fb2d16c70389b3464b84987276d.png)
>?如果之前本身也是用 PrometheusRule 定义的聚合规则，仍建议将其按照上述方式进行迁移，如果直接用 YAML 方式在集群中创建 PrometheusRule 资源，云原生监控暂时无法将其显示到控制台。



### 迁移告警配置


本文提供以下 Prometheus 告警原始配置 YAML 文件为例，介绍如何将其转换为云原生监控上类似的监控配置。
<dx-codeblock>
:::  yaml
  - alert: NodeNotReady
    expr: kube_node_status_condition{condition="Ready",status="true"} == 0
    for: 5m
    labels: 
      severity: critical
    annotations: 
      description: 节点 {{ $labels.node }} 长时间不可用 (集群id {{ $labels.cluster }})
:::
</dx-codeblock>

1. 登录 [容器服务控制台](https://console.cloud.tencent.com/tke2)。
2. 在左侧菜单栏中单击 【云原生监控】进入云原生监控页面。
3. 单击需要配置的云原生监控 ID/名称，进入基本页面。
4. 选择 【告警配置】>【新建告警策略】，配置告警策略：
![](https://main.qcloudimg.com/raw/bf30bd3141f872415425c537becfeb9c.png)
	-  **PromQL**：等同于原始配置的 expr 字段，也是告警的核心配置，是用于指示告警触发条件的 PromQL 表达式。
	- **Labels**：等同于原始配置的 labels，为告警添加额外的 label。
	- **告警内容**：表示推送的告警内容，通常使用模板，插入一些变量，通常建议带上集群 id，`{{ $labels.cluster }}` 就表示集群 id。
	- **持续时间**：等同于原始配置的 `for`，表示达到告警条件多久之后还没恢复就推送告警。本文示例配置为5分钟。
	- **收敛时间**：等同于 AlertManager 的 [repeat_interval](https://prometheus.io/docs/alerting/latest/configuration/#route) 配置，表示某个告警推送之后多久之后还未恢复就再次推送，即相同告警的推送间隔时长。本文示例配置为1小时。
>?上述告警配置示例表示节点状态变为 NotReady 之后，5分钟内未恢复即推送告警，如果长时间未恢复，则间隔1小时再次推送告警。
5. 配置告警渠道，目前支持腾讯云与 WebHook 两类:
<dx-tabs>
::: 腾讯云告警渠道
腾讯云告警渠道集成以下几种告警方式，可根据自身需求勾选：
- 短信
- 邮件
- 微信（关注腾讯云公众号后才能接受告警通知）
- 电话
![](https://main.qcloudimg.com/raw/91e1df76edeb357a926b6c497417aa77.png)
:::
::: WebHook\s告警渠道
如要更多告警渠道，例如钉钉、Zoom 等，可自行部署相关的 WebHook 后端，并在云原生监控指定 WebHook 的 URL:
![](https://main.qcloudimg.com/raw/f4c640ac5317dd3ca7d6b110b8f56ffa.png)
:::
</dx-tabs>
6. 上面示例告警的微信推送效果如下图所示：
<img src="https://main.qcloudimg.com/raw/fc61cdd39d36844100a6b64833ea92ef.png" width="80%"></img><br>





### 迁移 Grafana 面板

自建 Prometheus 往往积累了许多自定义的 Grafana 监控面板，如需迁移到其他平台，在面板数量较多的情况下，挨个导出再导入方式效率太低。借助 [grafana-backup](https://github.com/ysde/grafana-backup-tool) 工具可以实现 grafana 面板的批量导出和导入，您可以参考以下批量导出导入面板的方法进行快速迁移。

1. 执行以下命令安装下 grafana-backup。
```bash
pip3 install grafana-backup
```
 >?推荐使用 Python3，使用 Python2 可能存在兼容性问题。
2. 创建 API Keys：
	1. 分别打开自建 Grafana 与云原生监控 Grafana 的配置面板，选择【API Keys】>【New API Key】，如下图所示：
	![](https://main.qcloudimg.com/raw/b4800bdefcb3b644cbe327247c28eff9.png)
	2. 在弹出的 Add API Key 窗口中，创建一个 admin 的 APIKey，如下图所示：
	![](https://main.qcloudimg.com/raw/3f9c4e6f40374c47cf9ae0c7b982c1fa.png)
4. 为需要导出的面板准备备份配置文件:
	1. 执行以下命令，获取自建 Grafana 的访问地址。示例如下：
	 ``` bash
	 $ kubectl -n  monitoring get svc
	 NAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
	 grafana                 ClusterIP   172.21.254.127   <none>        3000/TCP                     25h
	```
	 >?示例中 Grafana 的集群内访问地址为 `http://172.21.254.127:3000`。
	2. 准备 grafana-backup 配置文件 (写入 Grafana 地址与 APIKey)。示例如下：
	```bash
	export TOKEN=<TOKEN>
	cat > ~/.grafana-backup.json <<EOF
	{
	 "general": {
		 "debug": true,
		 "backup_dir": "_OUTPUT_"
	 },
	 "grafana": {
		 "url": "http://172.21.254.127:3000",
		 "token": "${TOKEN}"
	 }
	}
	EOF
	```
	 >? &lt;TOKEN> 需要替换为自建 Grafana 的 APIKey，url 地址需替换为实际环境中的地址。
	3. 执行以下命令将所有面板全部导出。示例如下：
	```bash
	grafana-backup save
	```
	面板将以一个压缩文件的形式保存在 `_OUTPUT_` 目录下，您可以执行以下命令查看该目录下存在的文件。示例如下：
	```bash
	$ tree _OUTPUT_
	_OUTPUT_
	└── 202012151049.tar.gz

	0 directories, 1 file
	```
5. 执行以下命令准备还原配置文件，示例如下：
```bash
export TOKEN=<TOKEN>
cat > ~/.grafana-backup.json <<EOF
{
  "general": {
    "debug": true,
    "backup_dir": "_OUTPUT_"
  },
  "grafana": {
    "url": "http://prom-xxxxxx-grafana.ccs.tencent-cloud.com",
    "token": "${TOKEN}"
  }
}
EOF
```
 >?将 &lt;TOKEN> 替换为云原生监控 Grafana 的 APIKey，url 替换为云原生监控 Grafana 的访问地址（通常用外网访问地址，需开启）。
6. 执行以下命令，将导出的面板一键导入到云原生监控 Grafana。示例如下：
``` bash
grafana-backup restore _OUTPUT_/202012151049.tar.gz
```
7. 在 Grafana 配置面板选择 【Dashboard settings】>【Variables】>【New】， 新建 `cluster` 字段。建议为所有面板都加上 `cluster` 的过滤字段，云原生监控支持多集群，将会给每个集群的数据打上 `cluster` 标签，用集群 ID 来区分不同集群。如下图所示：
![](https://main.qcloudimg.com/raw/4fc513c387eac66d7e9dbb6d0ee9ad1f.png)
>?label_values 中填入当前面板任意涉及到的一个指标名即可（示例中为 node_uname_info）。
8. 修改所有面板中的 PromQL 查询语句，加入 `cluster=~"$cluster"` 过滤条件。如下图所示：
![](https://main.qcloudimg.com/raw/2fdd3bb6dfd4e59072c0bb4b41d2e6c5.png)




### 与现有系统集成


云原生监控支持集成到自建 Grafana 和 AlertManager 系统：

<dx-tabs>
::: 接入自建\sGrafana
云原生监控提供 Prometheus 的 API，如需使用自建的 Grafana 来展示监控，可以将云原生监控的数据作为一个 Prometheus 数据源添加到自建 Grafana，Prometheus API 的地址可在 TKE 控制台集群基本信息中查到。
1. 获取 Prometheus API 地址。
![](https://main.qcloudimg.com/raw/c42965924e1ace83249e0b181880fada.png)
 >?确保自建的 Grafana 与云原生监控在同一私有网络 VPC 下或两者网络已打通。
2. 在 Grafana 中添加 Prometheus API 地址作为 Prometheus 数据源。如下图所示：
![](https://main.qcloudimg.com/raw/70dfd95e69c3764c62dc874dfbb8ff68.png)
:::
::: 接入自建\sAlertManager
如需实现更复杂的告警需求，或者期望使用内部自建的 AlertManager 来统一告警，可以选择让云原生监控的告警接入自建的 AlertManager，只需在创建云原生监控实例的时候，在高级设置里填入自建 AlertManager 的地址即可:
![](https://main.qcloudimg.com/raw/72fa6c36807d366ca4a96059e2cdd51c.png)
:::
</dx-tabs>

