模型服务列表页面显示了当前各个服务的运行状态、占用资源、调用地址以及其他信息。单击【模型调用】，可获得当前服务的访问地址 IP 和密钥 Token。
![](https://main.qcloudimg.com/raw/dfe11a2e277db8e307bc3ffbe2d5470a.png)
![](https://main.qcloudimg.com/raw/02af42f7c9fca5fabf8e39f04661458c.jpg)

## 调用示例
本文通过示例展示服务调用方式，相关资料参见 [经典深度学习 inception 模型](http://ti-ems-1255502019.cosbj.myqcloud.com/tfserving/inception/1556451336.zip)。
- **模型运行环境：**tfserving 环境。
- **模型地址：**cos://data-122232422.cos.ap-bejing.myqcloud.com/tfserving/inception/155645133 
- **资源配置：**1CPU 核 2G 内存。
- **扩缩容配置：** CPU 利用率目标60%，内存利用率目标60%；最小实例数为1，最大实例数为5。[下载测试图片](http://ti-ems-1255502019.cosbj.myqcloud.com/tfserving/inception/image.encode)

## 本地测试示例
- 模型默认暴露两个端口：80端口提供 HTTP 服务，9000端口提供 grpc 服务。模型服务的预测访问路径默认为`/v1/models/m:predict`。
- 本地使用 curl 或者其他 rest 测试工具进行测试，这里我们使用 curl 进行测试，将获取的 token 填写为`header: X-Auth-Token`。
```
curl -H “Content-Type: application/json” \
-H "x-Auth-Token: TOKEN" \
-X POST IP/v1/models/m:predict -d "@image.encode"
```
上图例子中的 `image.encode `为训练生成的 inception 模型定义的 JSON 数据格式：`{"instances":[{"b64": 图片 base64 编码}]}`。这里我们使用上面的测试图片进行编码。
可以看出模型服务运行正常，正确的对发送的花朵进行了分类。
```
{
    "predictions": [
        {
            "class_idx": 2,
            "probabilities": [0.00685295, 0.00298387, 0.965622, 0.0072101, 0.0173306],
            "classes": "roses"
        }
    ]
}
```
